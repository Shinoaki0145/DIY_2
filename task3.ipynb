{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4cdf6d06",
   "metadata": {},
   "source": [
    "# Task 3: Model Evaluation\n",
    "This notebook evaluates the trained models using comprehensive metrics and visualizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed423650",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    confusion_matrix, classification_report, roc_auc_score, roc_curve\n",
    ")\n",
    "import pickle\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set visualization style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d25c7227",
   "metadata": {},
   "source": [
    "## 1. Load Trained Models and Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "863f9b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load trained models\n",
    "with open('trained_models.pkl', 'rb') as f:\n",
    "    trained_models = pickle.load(f)\n",
    "\n",
    "# Load preprocessing objects\n",
    "with open('preprocessing_objects.pkl', 'rb') as f:\n",
    "    preprocessing_objects = pickle.load(f)\n",
    "\n",
    "# Load train/test split\n",
    "with open('split_data.pkl', 'rb') as f:\n",
    "    split_data = pickle.load(f)\n",
    "\n",
    "# Extract data\n",
    "X_train = split_data['X_train']\n",
    "X_test = split_data['X_test']\n",
    "X_train_scaled = split_data['X_train_scaled']\n",
    "X_test_scaled = split_data['X_test_scaled']\n",
    "y_train = split_data['y_train']\n",
    "y_test = split_data['y_test']\n",
    "\n",
    "# Extract preprocessing objects\n",
    "label_encoder_target = preprocessing_objects['label_encoder_target']\n",
    "target_mapping = preprocessing_objects['target_mapping']\n",
    "\n",
    "print(\"Data loaded successfully!\")\n",
    "print(f\"Number of models: {len(trained_models)}\")\n",
    "print(f\"Test set size: {len(y_test)} samples\")\n",
    "print(f\"\\nModels available: {list(trained_models.keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e57fd72a",
   "metadata": {},
   "source": [
    "## 2. Evaluate All Models on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1caeebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate all models\n",
    "results = []\n",
    "\n",
    "for name, model in trained_models.items():\n",
    "    print(f\"Evaluating {name}...\")\n",
    "    \n",
    "    # Use scaled data for models that were trained on scaled data\n",
    "    if name in ['Logistic Regression', 'K-Nearest Neighbors', 'Support Vector Machine', 'Naive Bayes']:\n",
    "        y_pred = model.predict(X_test_scaled)\n",
    "        y_train_pred = model.predict(X_train_scaled)\n",
    "    else:\n",
    "        y_pred = model.predict(X_test)\n",
    "        y_train_pred = model.predict(X_train)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "    test_accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred, average='weighted')\n",
    "    recall = recall_score(y_test, y_pred, average='weighted')\n",
    "    f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "    \n",
    "    results.append({\n",
    "        'Model': name,\n",
    "        'Train Accuracy': train_accuracy,\n",
    "        'Test Accuracy': test_accuracy,\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'F1-Score': f1,\n",
    "        'Overfit': train_accuracy - test_accuracy\n",
    "    })\n",
    "\n",
    "# Create results dataframe\n",
    "results_df = pd.DataFrame(results).sort_values('Test Accuracy', ascending=False)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 100)\n",
    "print(\"MODEL EVALUATION RESULTS\")\n",
    "print(\"=\" * 100)\n",
    "print(results_df.to_string(index=False))\n",
    "print(\"=\" * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39ad1f26",
   "metadata": {},
   "source": [
    "## 3. Visualize Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "104c1f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Test Accuracy comparison\n",
    "plt.figure(figsize=(14, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.barh(results_df['Model'], results_df['Test Accuracy'], color='lightblue')\n",
    "plt.xlabel('Test Accuracy')\n",
    "plt.title('Model Test Accuracy Comparison')\n",
    "plt.xlim([0, 1])\n",
    "for i, v in enumerate(results_df['Test Accuracy']):\n",
    "    plt.text(v + 0.01, i, f'{v:.4f}', va='center')\n",
    "\n",
    "# Plot multiple metrics comparison\n",
    "plt.subplot(1, 2, 2)\n",
    "x = np.arange(len(results_df))\n",
    "width = 0.2\n",
    "plt.bar(x - width*1.5, results_df['Test Accuracy'], width, label='Accuracy', color='lightblue')\n",
    "plt.bar(x - width*0.5, results_df['Precision'], width, label='Precision', color='lightgreen')\n",
    "plt.bar(x + width*0.5, results_df['Recall'], width, label='Recall', color='lightcoral')\n",
    "plt.bar(x + width*1.5, results_df['F1-Score'], width, label='F1-Score', color='lightyellow')\n",
    "plt.xlabel('Model')\n",
    "plt.ylabel('Score')\n",
    "plt.title('Model Performance Metrics')\n",
    "plt.xticks(x, results_df['Model'], rotation=45, ha='right')\n",
    "plt.legend()\n",
    "plt.ylim([0, 1])\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "790186c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot overfitting analysis\n",
    "plt.figure(figsize=(12, 6))\n",
    "x = np.arange(len(results_df))\n",
    "plt.bar(x - 0.2, results_df['Train Accuracy'], 0.4, label='Train Accuracy', color='skyblue')\n",
    "plt.bar(x + 0.2, results_df['Test Accuracy'], 0.4, label='Test Accuracy', color='coral')\n",
    "plt.xlabel('Model')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Training vs Testing Accuracy (Overfitting Analysis)')\n",
    "plt.xticks(x, results_df['Model'], rotation=45, ha='right')\n",
    "plt.legend()\n",
    "plt.ylim([0, 1])\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nOverfitting Analysis:\")\n",
    "print(\"=\" * 60)\n",
    "for _, row in results_df.iterrows():\n",
    "    status = \"Good\" if row['Overfit'] < 0.05 else \"Moderate\" if row['Overfit'] < 0.1 else \"High\"\n",
    "    print(f\"{row['Model']:25s}: Overfit = {row['Overfit']:.4f} ({status})\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1df1010b",
   "metadata": {},
   "source": [
    "## 4. Confusion Matrix for Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25e79b9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get best model\n",
    "best_model_name = results_df.iloc[0]['Model']\n",
    "best_model = trained_models[best_model_name]\n",
    "\n",
    "print(f\"Best Model: {best_model_name}\")\n",
    "print(f\"Test Accuracy: {results_df.iloc[0]['Test Accuracy']:.4f}\")\n",
    "\n",
    "# Make predictions\n",
    "if best_model_name in ['Logistic Regression', 'K-Nearest Neighbors', 'Support Vector Machine', 'Naive Bayes']:\n",
    "    y_pred = best_model.predict(X_test_scaled)\n",
    "else:\n",
    "    y_pred = best_model.predict(X_test)\n",
    "\n",
    "# Calculate confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Get class labels\n",
    "class_names = [label_encoder_target.inverse_transform([i])[0] for i in range(len(label_encoder_target.classes_))]\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=class_names, yticklabels=class_names)\n",
    "plt.title(f'Confusion Matrix - {best_model_name}')\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.yticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5978eb3",
   "metadata": {},
   "source": [
    "## 5. Detailed Classification Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dac7ccf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print detailed classification report for best model\n",
    "print(f\"\\nDetailed Classification Report for {best_model_name}:\")\n",
    "print(\"=\" * 80)\n",
    "print(classification_report(y_test, y_pred, target_names=class_names))\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6d0ecec",
   "metadata": {},
   "source": [
    "## 6. Feature Importance (for tree-based models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9879414a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance for tree-based models\n",
    "tree_models = ['Decision Tree', 'Random Forest', 'Gradient Boosting']\n",
    "\n",
    "for model_name in tree_models:\n",
    "    if model_name in trained_models:\n",
    "        model = trained_models[model_name]\n",
    "        \n",
    "        # Get feature importances\n",
    "        importances = model.feature_importances_\n",
    "        feature_names = preprocessing_objects['feature_names']\n",
    "        \n",
    "        # Create dataframe\n",
    "        feature_importance_df = pd.DataFrame({\n",
    "            'Feature': feature_names,\n",
    "            'Importance': importances\n",
    "        }).sort_values('Importance', ascending=False)\n",
    "        \n",
    "        # Plot feature importance\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.barh(feature_importance_df['Feature'], feature_importance_df['Importance'], color='teal')\n",
    "        plt.xlabel('Importance')\n",
    "        plt.title(f'Feature Importance - {model_name}')\n",
    "        plt.gca().invert_yaxis()\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        print(f\"\\nTop 10 Important Features - {model_name}:\")\n",
    "        print(\"=\" * 60)\n",
    "        print(feature_importance_df.head(10).to_string(index=False))\n",
    "        print(\"=\" * 60)\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04392745",
   "metadata": {},
   "source": [
    "## 7. Model Comparison Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "021c04d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics\n",
    "print(\"\\n\" + \"=\" * 100)\n",
    "print(\"FINAL SUMMARY\")\n",
    "print(\"=\" * 100)\n",
    "print(f\"\\n1. BEST MODEL: {best_model_name}\")\n",
    "print(f\"   - Test Accuracy: {results_df.iloc[0]['Test Accuracy']:.4f}\")\n",
    "print(f\"   - F1-Score: {results_df.iloc[0]['F1-Score']:.4f}\")\n",
    "print(f\"   - Precision: {results_df.iloc[0]['Precision']:.4f}\")\n",
    "print(f\"   - Recall: {results_df.iloc[0]['Recall']:.4f}\")\n",
    "\n",
    "print(f\"\\n2. MODEL RANKINGS (by Test Accuracy):\")\n",
    "for idx, row in results_df.iterrows():\n",
    "    print(f\"   {row.name + 1}. {row['Model']:30s} - Test Accuracy: {row['Test Accuracy']:.4f}\")\n",
    "\n",
    "print(f\"\\n3. AVERAGE PERFORMANCE ACROSS ALL MODELS:\")\n",
    "print(f\"   - Average Test Accuracy: {results_df['Test Accuracy'].mean():.4f}\")\n",
    "print(f\"   - Average F1-Score: {results_df['F1-Score'].mean():.4f}\")\n",
    "print(f\"   - Average Precision: {results_df['Precision'].mean():.4f}\")\n",
    "print(f\"   - Average Recall: {results_df['Recall'].mean():.4f}\")\n",
    "\n",
    "print(f\"\\n4. BEST BALANCED MODEL (lowest overfitting with good accuracy):\")\n",
    "balanced_df = results_df[results_df['Test Accuracy'] > 0.9].sort_values('Overfit')\n",
    "if len(balanced_df) > 0:\n",
    "    print(f\"   - {balanced_df.iloc[0]['Model']}\")\n",
    "    print(f\"   - Test Accuracy: {balanced_df.iloc[0]['Test Accuracy']:.4f}\")\n",
    "    print(f\"   - Overfit: {balanced_df.iloc[0]['Overfit']:.4f}\")\n",
    "else:\n",
    "    print(\"   - No model meets the criteria (Test Accuracy > 0.9)\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 100)\n",
    "print(\"EVALUATION COMPLETE!\")\n",
    "print(\"=\" * 100)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
