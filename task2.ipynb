{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b4b59a88",
   "metadata": {},
   "source": [
    "# Task 2: Model Training\n",
    "This notebook implements data preprocessing, feature engineering, and model training for obesity classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4f92470",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1cc25e9",
   "metadata": {},
   "source": [
    "## 1. Load and Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13f4b4df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_csv('ObesityDataset.csv')\n",
    "\n",
    "print(\"Dataset loaded successfully!\")\n",
    "print(f\"Shape: {df.shape}\")\n",
    "print(f\"\\nTarget classes: {df['NObeyesdad'].unique()}\")\n",
    "print(f\"\\nClass distribution:\")\n",
    "print(df['NObeyesdad'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b3535ad",
   "metadata": {},
   "source": [
    "## 2. Data Preprocessing and Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bf38089",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a copy for preprocessing\n",
    "df_processed = df.copy()\n",
    "\n",
    "# Encode target variable\n",
    "label_encoder_target = LabelEncoder()\n",
    "df_processed['NObeyesdad_encoded'] = label_encoder_target.fit_transform(df_processed['NObeyesdad'])\n",
    "\n",
    "# Store the mapping\n",
    "target_mapping = dict(zip(label_encoder_target.classes_, label_encoder_target.transform(label_encoder_target.classes_)))\n",
    "print(\"Target variable encoding:\")\n",
    "for original, encoded in sorted(target_mapping.items(), key=lambda x: x[1]):\n",
    "    print(f\"  {encoded}: {original}\")\n",
    "\n",
    "# Separate features and target\n",
    "X = df_processed.drop(['NObeyesdad', 'NObeyesdad_encoded'], axis=1)\n",
    "y = df_processed['NObeyesdad_encoded']\n",
    "\n",
    "print(f\"\\nFeature matrix shape: {X.shape}\")\n",
    "print(f\"Target vector shape: {y.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4626dc28",
   "metadata": {},
   "source": [
    "### 2.1 Encode Categorical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea4e854f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify categorical columns\n",
    "categorical_cols = X.select_dtypes(include=['object']).columns.tolist()\n",
    "print(f\"Categorical columns: {categorical_cols}\")\n",
    "\n",
    "# Encode categorical features\n",
    "label_encoders = {}\n",
    "X_encoded = X.copy()\n",
    "\n",
    "for col in categorical_cols:\n",
    "    le = LabelEncoder()\n",
    "    X_encoded[col] = le.fit_transform(X[col])\n",
    "    label_encoders[col] = le\n",
    "    print(f\"\\n{col} encoding:\")\n",
    "    for original, encoded in zip(le.classes_, le.transform(le.classes_)):\n",
    "        print(f\"  {encoded}: {original}\")\n",
    "\n",
    "print(f\"\\nEncoded feature matrix shape: {X_encoded.shape}\")\n",
    "print(\"\\nFirst few rows after encoding:\")\n",
    "print(X_encoded.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c36ef27a",
   "metadata": {},
   "source": [
    "## 3. Split Data into Training and Testing Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c63885af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data (80% training, 20% testing)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_encoded, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Training set size: {X_train.shape[0]} samples\")\n",
    "print(f\"Testing set size: {X_test.shape[0]} samples\")\n",
    "print(f\"\\nTraining set class distribution:\")\n",
    "print(y_train.value_counts().sort_index())\n",
    "print(f\"\\nTesting set class distribution:\")\n",
    "print(y_test.value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98682d86",
   "metadata": {},
   "source": [
    "## 4. Feature Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "147483fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale features (important for distance-based algorithms)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(\"Feature scaling completed!\")\n",
    "print(f\"\\nScaled training set shape: {X_train_scaled.shape}\")\n",
    "print(f\"Scaled testing set shape: {X_test_scaled.shape}\")\n",
    "print(f\"\\nFeature means after scaling (should be close to 0):\")\n",
    "print(np.mean(X_train_scaled, axis=0))\n",
    "print(f\"\\nFeature stds after scaling (should be close to 1):\")\n",
    "print(np.std(X_train_scaled, axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38ef4714",
   "metadata": {},
   "source": [
    "## 5. Train Multiple Classification Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98d83398",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define multiple models to train\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(max_iter=1000, random_state=42),\n",
    "    'Decision Tree': DecisionTreeClassifier(random_state=42),\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    'Gradient Boosting': GradientBoostingClassifier(n_estimators=100, random_state=42),\n",
    "    'K-Nearest Neighbors': KNeighborsClassifier(n_neighbors=5),\n",
    "    'Support Vector Machine': SVC(kernel='rbf', random_state=42),\n",
    "    'Naive Bayes': GaussianNB()\n",
    "}\n",
    "\n",
    "print(f\"Training {len(models)} models...\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df63a68b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train all models and store them\n",
    "trained_models = {}\n",
    "training_scores = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"\\nTraining {name}...\")\n",
    "    \n",
    "    # Use scaled data for models that benefit from scaling\n",
    "    if name in ['Logistic Regression', 'K-Nearest Neighbors', 'Support Vector Machine', 'Naive Bayes']:\n",
    "        model.fit(X_train_scaled, y_train)\n",
    "        train_score = model.score(X_train_scaled, y_train)\n",
    "    else:\n",
    "        model.fit(X_train, y_train)\n",
    "        train_score = model.score(X_train, y_train)\n",
    "    \n",
    "    trained_models[name] = model\n",
    "    training_scores[name] = train_score\n",
    "    \n",
    "    print(f\"  Training accuracy: {train_score:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"All models trained successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66ec8834",
   "metadata": {},
   "source": [
    "## 6. Training Accuracy Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "001e3c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a summary of training accuracies\n",
    "training_results = pd.DataFrame({\n",
    "    'Model': list(training_scores.keys()),\n",
    "    'Training Accuracy': list(training_scores.values())\n",
    "}).sort_values('Training Accuracy', ascending=False)\n",
    "\n",
    "print(\"\\nTraining Accuracy Summary:\")\n",
    "print(\"=\" * 60)\n",
    "print(training_results.to_string(index=False))\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Visualize training accuracies\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.barh(training_results['Model'], training_results['Training Accuracy'], color='skyblue')\n",
    "plt.xlabel('Training Accuracy')\n",
    "plt.title('Model Training Accuracy Comparison')\n",
    "plt.xlim([0, 1])\n",
    "for i, v in enumerate(training_results['Training Accuracy']):\n",
    "    plt.text(v + 0.01, i, f'{v:.4f}', va='center')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce52fe51",
   "metadata": {},
   "source": [
    "## 7. Save Models and Preprocessing Objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5731e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save models and preprocessing objects for use in Task 3\n",
    "import pickle\n",
    "\n",
    "# Save all trained models\n",
    "with open('trained_models.pkl', 'wb') as f:\n",
    "    pickle.dump(trained_models, f)\n",
    "\n",
    "# Save preprocessing objects\n",
    "preprocessing_objects = {\n",
    "    'label_encoders': label_encoders,\n",
    "    'label_encoder_target': label_encoder_target,\n",
    "    'scaler': scaler,\n",
    "    'target_mapping': target_mapping,\n",
    "    'feature_names': X_encoded.columns.tolist()\n",
    "}\n",
    "\n",
    "with open('preprocessing_objects.pkl', 'wb') as f:\n",
    "    pickle.dump(preprocessing_objects, f)\n",
    "\n",
    "# Save train/test split for evaluation\n",
    "split_data = {\n",
    "    'X_train': X_train,\n",
    "    'X_test': X_test,\n",
    "    'X_train_scaled': X_train_scaled,\n",
    "    'X_test_scaled': X_test_scaled,\n",
    "    'y_train': y_train,\n",
    "    'y_test': y_test\n",
    "}\n",
    "\n",
    "with open('split_data.pkl', 'wb') as f:\n",
    "    pickle.dump(split_data, f)\n",
    "\n",
    "print(\"✓ Models saved to 'trained_models.pkl'\")\n",
    "print(\"✓ Preprocessing objects saved to 'preprocessing_objects.pkl'\")\n",
    "print(\"✓ Train/test split saved to 'split_data.pkl'\")\n",
    "print(\"\\nAll data ready for evaluation in Task 3!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "017135ee",
   "metadata": {},
   "source": [
    "## Section 8 — Evaluation\n",
    "\n",
    "This section computes the required evaluation metrics (Accuracy, Confusion Matrix, Macro Precision, Macro Recall, Macro F1-score, Macro ROC–AUC), saves per-model predictions/probabilities, writes confusion matrix plots, exports `pipelines/results_summary_eval.csv`, and prepares deployment artifacts (`pipelines/best_model.joblib`, `pipelines/feature_order.json`, `pipelines/target_mapping.json`, and `pipelines/task3.py`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78f4b2fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 8 — Evaluation (compute required metrics, save predictions/probas, and prepare deployment artifacts)\n",
    "import os\n",
    "import json\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, roc_auc_score\n",
    "from sklearn.preprocessing import label_binarize\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "os.makedirs('pipelines', exist_ok=True)\n",
    "\n",
    "# Helper: try to get variables from namespace or fall back to saved files\n",
    "try:\n",
    "    models_dict = trained_models\n",
    "    print('Using in-memory trained models')\n",
    "except NameError:\n",
    "    import pickle\n",
    "    with open('trained_models.pkl','rb') as f:\n",
    "        models_dict = pickle.load(f)\n",
    "    print('Loaded models from trained_models.pkl')\n",
    "\n",
    "try:\n",
    "    X_test\n",
    "    y_test\n",
    "    X_test_scaled\n",
    "    print('Using in-memory split data')\n",
    "except NameError:\n",
    "    import pickle\n",
    "    with open('split_data.pkl','rb') as f:\n",
    "        split_data = pickle.load(f)\n",
    "    X_test = split_data.get('X_test')\n",
    "    y_test = split_data.get('y_test')\n",
    "    X_test_scaled = split_data.get('X_test_scaled')\n",
    "    print('Loaded split_data.pkl')\n",
    "\n",
    "# Try to load preprocessing objects if available\n",
    "preproc_obj = None\n",
    "if os.path.exists('preprocessing_objects.pkl'):\n",
    "    import pickle\n",
    "    with open('preprocessing_objects.pkl','rb') as f:\n",
    "        preproc_obj = pickle.load(f)\n",
    "    print('Loaded preprocessing_objects.pkl')\n",
    "\n",
    "# Classes and number of classes\n",
    "if hasattr(list(models_dict.values())[0], 'classes_'):\n",
    "    clf_classes = list(models_dict.values())[0].classes_\n",
    "elif preproc_obj and 'label_encoder_target' in preproc_obj:\n",
    "    clf_classes = preproc_obj['label_encoder_target'].classes_\n",
    "else:\n",
    "    # fallback: infer from y_test\n",
    "    clf_classes = sorted(y_test.unique()) if hasattr(y_test, 'unique') else list(sorted(set(y_test)))\n",
    "\n",
    "n_classes = len(clf_classes)\n",
    "print('Detected classes:', clf_classes)\n",
    "\n",
    "# Define which models were trained on scaled data in this notebook (mirror training logic)\n",
    "scaled_models = ['Logistic Regression', 'K-Nearest Neighbors', 'Support Vector Machine', 'Naive Bayes']\n",
    "\n",
    "results = []\n",
    "\n",
    "for name, model in models_dict.items():\n",
    "    print(f\"Evaluating {name} ...\")\n",
    "\n",
    "    # Select appropriate test input (scaled vs unscaled) according to training logic\n",
    "    if name in scaled_models:\n",
    "        X_eval = X_test_scaled if 'X_test_scaled' in globals() and X_test_scaled is not None else X_test\n",
    "    else:\n",
    "        X_eval = X_test\n",
    "\n",
    "    # Predictions\n",
    "    try:\n",
    "        y_pred = model.predict(X_eval)\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to predict with {name}: {e}\")\n",
    "        continue\n",
    "\n",
    "    # Probabilities or decision scores\n",
    "    y_proba = None\n",
    "    y_score = None\n",
    "    if hasattr(model, 'predict_proba'):\n",
    "        try:\n",
    "            y_proba = model.predict_proba(X_eval)\n",
    "        except Exception:\n",
    "            y_proba = None\n",
    "    if y_proba is None and hasattr(model, 'decision_function'):\n",
    "        try:\n",
    "            y_score = model.decision_function(X_eval)\n",
    "        except Exception:\n",
    "            y_score = None\n",
    "\n",
    "    # Save predictions\n",
    "    preds_df = pd.DataFrame({'y_true': y_test.reset_index(drop=True), 'y_pred': y_pred})\n",
    "    preds_path = f\"pipelines/{name.replace(' ', '_')}_predictions.csv\"\n",
    "    preds_df.to_csv(preds_path, index=False)\n",
    "\n",
    "    # Save probabilities if available\n",
    "    if y_proba is not None:\n",
    "        proba_df = pd.DataFrame(y_proba, columns=[str(c) for c in (model.classes_ if hasattr(model, 'classes_') else clf_classes)])\n",
    "        proba_path = f\"pipelines/{name.replace(' ', '_')}_probas.csv\"\n",
    "        proba_df.to_csv(proba_path, index=False)\n",
    "    else:\n",
    "        proba_path = None\n",
    "\n",
    "    # Metrics\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    prec = precision_score(y_test, y_pred, average='macro', zero_division=0)\n",
    "    rec = recall_score(y_test, y_pred, average='macro', zero_division=0)\n",
    "    f1 = f1_score(y_test, y_pred, average='macro', zero_division=0)\n",
    "\n",
    "    # ROC-AUC (macro) using probabilities if available, else decision function if available\n",
    "    auc_macro = None\n",
    "    try:\n",
    "        y_test_bin = label_binarize(y_test, classes=range(n_classes))\n",
    "        if y_proba is not None:\n",
    "            auc_macro = roc_auc_score(y_test_bin, y_proba, average='macro', multi_class='ovr')\n",
    "        elif y_score is not None:\n",
    "            # decision function may return shape (n_samples, n_classes) for multiclass\n",
    "            auc_macro = roc_auc_score(y_test_bin, y_score, average='macro', multi_class='ovr')\n",
    "    except Exception as e:\n",
    "        auc_macro = None\n",
    "\n",
    "    # Confusion matrix and figure\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=clf_classes, yticklabels=clf_classes)\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.title(f'Confusion Matrix — {name}')\n",
    "    cm_path = f\"pipelines/{name.replace(' ', '_')}_confusion_matrix.png\"\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(cm_path)\n",
    "    plt.close()\n",
    "\n",
    "    results.append({\n",
    "        'Model': name,\n",
    "        'Accuracy': acc,\n",
    "        'Precision_macro': prec,\n",
    "        'Recall_macro': rec,\n",
    "        'F1_macro': f1,\n",
    "        'ROC_AUC_macro': auc_macro,\n",
    "        'Predictions_CSV': preds_path,\n",
    "        'Probas_CSV': proba_path,\n",
    "        'Confusion_Matrix_PNG': cm_path\n",
    "    })\n",
    "    print(f\"  Saved predictions to {preds_path}\")\n",
    "    if proba_path:\n",
    "        print(f\"  Saved probabilities to {proba_path}\")\n",
    "\n",
    "# Summary CSV\n",
    "results_df = pd.DataFrame(results).sort_values(['F1_macro','ROC_AUC_macro'], ascending=False, na_position='last')\n",
    "results_df.to_csv('pipelines/results_summary_eval.csv', index=False)\n",
    "print('\\nSaved evaluation summary to pipelines/results_summary_eval.csv')\n",
    "print(results_df)\n",
    "\n",
    "# Save the best model for deployment (choose by F1_macro primary)\n",
    "best_row = results_df.iloc[0]\n",
    "best_name = best_row['Model']\n",
    "best_model = models_dict[best_name]\n",
    "\n",
    "# Save best model and copy preprocessing objects\n",
    "import joblib\n",
    "joblib.dump(best_model, 'pipelines/best_model.joblib')\n",
    "if preproc_obj is not None:\n",
    "    with open('pipelines/preprocessing_objects.pkl','wb') as f:\n",
    "        import pickle\n",
    "        pickle.dump(preproc_obj, f)\n",
    "\n",
    "# Save feature order and target mapping for deployment\n",
    "feature_order = list(X_test.columns)\n",
    "with open('pipelines/feature_order.json','w') as f:\n",
    "    json.dump(feature_order, f)\n",
    "\n",
    "# target mapping: try to get from label encoder\n",
    "target_mapping = None\n",
    "if preproc_obj and 'label_encoder_target' in preproc_obj:\n",
    "    le = preproc_obj['label_encoder_target']\n",
    "    target_mapping = {str(i): str(c) for i, c in enumerate(le.classes_)}\n",
    "else:\n",
    "    try:\n",
    "        unique_classes = sorted(y_test.unique()) if hasattr(y_test, 'unique') else list(sorted(set(y_test)))\n",
    "        target_mapping = {str(i): str(c) for i,c in enumerate(unique_classes)}\n",
    "    except Exception:\n",
    "        target_mapping = None\n",
    "\n",
    "if target_mapping:\n",
    "    with open('pipelines/target_mapping.json','w') as f:\n",
    "        json.dump(target_mapping, f)\n",
    "\n",
    "print(f\"Saved best model: pipelines/best_model.joblib (model: {best_name})\")\n",
    "print('Saved feature_order.json and target_mapping.json for deployment')\n",
    "\n",
    "# Write a minimal Gradio app template (task3.py) into pipelines/\n",
    "app_template = r\"\"\"\n",
    "import joblib\n",
    "import json\n",
    "import pandas as pd\n",
    "import gradio as gr\n",
    "\n",
    "# Load artifacts\n",
    "model = joblib.load('pipelines/best_model.joblib')\n",
    "with open('pipelines/feature_order.json') as f:\n",
    "    feature_order = json.load(f)\n",
    "with open('pipelines/target_mapping.json') as f:\n",
    "    target_mapping = json.load(f)\n",
    "\n",
    "# If preprocessing_objects.pkl exists, load it to apply the same preprocessing used in training\n",
    "try:\n",
    "    import pickle\n",
    "    with open('pipelines/preprocessing_objects.pkl', 'rb') as f:\n",
    "        preproc = pickle.load(f)\n",
    "except Exception:\n",
    "    preproc = None\n",
    "\n",
    "classes = list(target_mapping.values())\n",
    "\n",
    "def preprocess_input(row: pd.DataFrame):\n",
    "    '''Apply the same transformations used during training if preprocessing objects are available.'''\n",
    "    df = row.copy()\n",
    "    if preproc is not None:\n",
    "        # Apply label encoders if available\n",
    "        les = preproc.get('label_encoders')\n",
    "        scaler = preproc.get('scaler')\n",
    "        if les:\n",
    "            for col, le in les.items():\n",
    "                if col in df:\n",
    "                    df[col] = le.transform(df[col])\n",
    "        if scaler is not None:\n",
    "            # assume scaler was fit on numeric columns only\n",
    "            numeric_cols = preproc.get('feature_names')\n",
    "            # try to extract numeric subset and scale\n",
    "            # Note: this template assumes the same feature ordering expected by the model\n",
    "            # For robust deployment, retrain and save a scikit-learn Pipeline that includes preprocessing.\n",
    "            try:\n",
    "                num_df = df.select_dtypes(include=['number'])\n",
    "                df[num_df.columns] = scaler.transform(num_df)\n",
    "            except Exception:\n",
    "                pass\n",
    "    return df\n",
    "\n",
    "\n",
    "def predict_fn(*args):\n",
    "    # Build single-row DataFrame\n",
    "    row = pd.DataFrame([dict(zip(feature_order, args))])\n",
    "    row_proc = preprocess_input(row)\n",
    "    pred = model.predict(row_proc)[0]\n",
    "    prob = None\n",
    "    if hasattr(model, 'predict_proba'):\n",
    "        prob = model.predict_proba(row_proc)[0]\n",
    "        prob = {c: float(p) for c,p in zip(classes, prob)}\n",
    "    return str(pred), prob\n",
    "\n",
    "# NOTE: Customize the inputs list below to match feature types (Dropdown vs Number)\n",
    "inputs = [gr.Textbox(label=f) for f in feature_order]\n",
    "outputs = [gr.Label(num_top_classes=1), gr.JSON()]\n",
    "\n",
    "iface = gr.Interface(fn=predict_fn, inputs=inputs, outputs=outputs, title='Obesity Level Prediction')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    iface.launch()\n",
    "\"\"\"\n",
    "\n",
    "with open('pipelines/task3.py','w', encoding='utf-8') as f:\n",
    "    f.write(app_template)\n",
    "\n",
    "print('Wrote pipelines/task3.py template for Gradio deployment')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
