{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b4b59a88",
   "metadata": {},
   "source": [
    "# Task 2: Model Training\n",
    "This notebook implements data preprocessing, feature engineering, and model training for obesity classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4f92470",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1cc25e9",
   "metadata": {},
   "source": [
    "## 1. Load and Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13f4b4df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_csv('ObesityDataset.csv')\n",
    "\n",
    "print(\"Dataset loaded successfully!\")\n",
    "print(f\"Shape: {df.shape}\")\n",
    "print(f\"\\nTarget classes: {df['NObeyesdad'].unique()}\")\n",
    "print(f\"\\nClass distribution:\")\n",
    "print(df['NObeyesdad'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b3535ad",
   "metadata": {},
   "source": [
    "## 2. Data Preprocessing and Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bf38089",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a copy for preprocessing\n",
    "df_processed = df.copy()\n",
    "\n",
    "# Encode target variable\n",
    "label_encoder_target = LabelEncoder()\n",
    "df_processed['NObeyesdad_encoded'] = label_encoder_target.fit_transform(df_processed['NObeyesdad'])\n",
    "\n",
    "# Store the mapping\n",
    "target_mapping = dict(zip(label_encoder_target.classes_, label_encoder_target.transform(label_encoder_target.classes_)))\n",
    "print(\"Target variable encoding:\")\n",
    "for original, encoded in sorted(target_mapping.items(), key=lambda x: x[1]):\n",
    "    print(f\"  {encoded}: {original}\")\n",
    "\n",
    "# Separate features and target\n",
    "X = df_processed.drop(['NObeyesdad', 'NObeyesdad_encoded'], axis=1)\n",
    "y = df_processed['NObeyesdad_encoded']\n",
    "\n",
    "print(f\"\\nFeature matrix shape: {X.shape}\")\n",
    "print(f\"Target vector shape: {y.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4626dc28",
   "metadata": {},
   "source": [
    "### 2.1 Encode Categorical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea4e854f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify categorical columns\n",
    "categorical_cols = X.select_dtypes(include=['object']).columns.tolist()\n",
    "print(f\"Categorical columns: {categorical_cols}\")\n",
    "\n",
    "# Encode categorical features\n",
    "label_encoders = {}\n",
    "X_encoded = X.copy()\n",
    "\n",
    "for col in categorical_cols:\n",
    "    le = LabelEncoder()\n",
    "    X_encoded[col] = le.fit_transform(X[col])\n",
    "    label_encoders[col] = le\n",
    "    print(f\"\\n{col} encoding:\")\n",
    "    for original, encoded in zip(le.classes_, le.transform(le.classes_)):\n",
    "        print(f\"  {encoded}: {original}\")\n",
    "\n",
    "print(f\"\\nEncoded feature matrix shape: {X_encoded.shape}\")\n",
    "print(\"\\nFirst few rows after encoding:\")\n",
    "print(X_encoded.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c36ef27a",
   "metadata": {},
   "source": [
    "## 3. Split Data into Training and Testing Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c63885af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data (80% training, 20% testing)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_encoded, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Training set size: {X_train.shape[0]} samples\")\n",
    "print(f\"Testing set size: {X_test.shape[0]} samples\")\n",
    "print(f\"\\nTraining set class distribution:\")\n",
    "print(y_train.value_counts().sort_index())\n",
    "print(f\"\\nTesting set class distribution:\")\n",
    "print(y_test.value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98682d86",
   "metadata": {},
   "source": [
    "## 4. Feature Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "147483fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale features (important for distance-based algorithms)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(\"Feature scaling completed!\")\n",
    "print(f\"\\nScaled training set shape: {X_train_scaled.shape}\")\n",
    "print(f\"Scaled testing set shape: {X_test_scaled.shape}\")\n",
    "print(f\"\\nFeature means after scaling (should be close to 0):\")\n",
    "print(np.mean(X_train_scaled, axis=0))\n",
    "print(f\"\\nFeature stds after scaling (should be close to 1):\")\n",
    "print(np.std(X_train_scaled, axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38ef4714",
   "metadata": {},
   "source": [
    "## 5. Train Multiple Classification Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98d83398",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define multiple models to train\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(max_iter=1000, random_state=42),\n",
    "    'Decision Tree': DecisionTreeClassifier(random_state=42),\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    'Gradient Boosting': GradientBoostingClassifier(n_estimators=100, random_state=42),\n",
    "    'K-Nearest Neighbors': KNeighborsClassifier(n_neighbors=5),\n",
    "    'Support Vector Machine': SVC(kernel='rbf', random_state=42),\n",
    "    'Naive Bayes': GaussianNB()\n",
    "}\n",
    "\n",
    "print(f\"Training {len(models)} models...\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df63a68b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train all models and store them\n",
    "trained_models = {}\n",
    "training_scores = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"\\nTraining {name}...\")\n",
    "    \n",
    "    # Use scaled data for models that benefit from scaling\n",
    "    if name in ['Logistic Regression', 'K-Nearest Neighbors', 'Support Vector Machine', 'Naive Bayes']:\n",
    "        model.fit(X_train_scaled, y_train)\n",
    "        train_score = model.score(X_train_scaled, y_train)\n",
    "    else:\n",
    "        model.fit(X_train, y_train)\n",
    "        train_score = model.score(X_train, y_train)\n",
    "    \n",
    "    trained_models[name] = model\n",
    "    training_scores[name] = train_score\n",
    "    \n",
    "    print(f\"  Training accuracy: {train_score:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"All models trained successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66ec8834",
   "metadata": {},
   "source": [
    "## 6. Training Accuracy Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "001e3c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a summary of training accuracies\n",
    "training_results = pd.DataFrame({\n",
    "    'Model': list(training_scores.keys()),\n",
    "    'Training Accuracy': list(training_scores.values())\n",
    "}).sort_values('Training Accuracy', ascending=False)\n",
    "\n",
    "print(\"\\nTraining Accuracy Summary:\")\n",
    "print(\"=\" * 60)\n",
    "print(training_results.to_string(index=False))\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Visualize training accuracies\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.barh(training_results['Model'], training_results['Training Accuracy'], color='skyblue')\n",
    "plt.xlabel('Training Accuracy')\n",
    "plt.title('Model Training Accuracy Comparison')\n",
    "plt.xlim([0, 1])\n",
    "for i, v in enumerate(training_results['Training Accuracy']):\n",
    "    plt.text(v + 0.01, i, f'{v:.4f}', va='center')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce52fe51",
   "metadata": {},
   "source": [
    "## 7. Save Models and Preprocessing Objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5731e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save models and preprocessing objects for use in Task 3\n",
    "import pickle\n",
    "\n",
    "# Save all trained models\n",
    "with open('trained_models.pkl', 'wb') as f:\n",
    "    pickle.dump(trained_models, f)\n",
    "\n",
    "# Save preprocessing objects\n",
    "preprocessing_objects = {\n",
    "    'label_encoders': label_encoders,\n",
    "    'label_encoder_target': label_encoder_target,\n",
    "    'scaler': scaler,\n",
    "    'target_mapping': target_mapping,\n",
    "    'feature_names': X_encoded.columns.tolist()\n",
    "}\n",
    "\n",
    "with open('preprocessing_objects.pkl', 'wb') as f:\n",
    "    pickle.dump(preprocessing_objects, f)\n",
    "\n",
    "# Save train/test split for evaluation\n",
    "split_data = {\n",
    "    'X_train': X_train,\n",
    "    'X_test': X_test,\n",
    "    'X_train_scaled': X_train_scaled,\n",
    "    'X_test_scaled': X_test_scaled,\n",
    "    'y_train': y_train,\n",
    "    'y_test': y_test\n",
    "}\n",
    "\n",
    "with open('split_data.pkl', 'wb') as f:\n",
    "    pickle.dump(split_data, f)\n",
    "\n",
    "print(\"✓ Models saved to 'trained_models.pkl'\")\n",
    "print(\"✓ Preprocessing objects saved to 'preprocessing_objects.pkl'\")\n",
    "print(\"✓ Train/test split saved to 'split_data.pkl'\")\n",
    "print(\"\\nAll data ready for evaluation in Task 3!\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
